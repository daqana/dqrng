---
title: "Fast sampling methods"
author: "Ralf Stubner"
date: "`r Sys.Date()`"
output: rmarkdown::html_vignette
bibliography: bibliography.bib
vignette: >
  %\VignetteIndexEntry{Fast sampling methods}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
evaluate <- FALSE
require(bench)
#knitr::write_bib("wrswoR", file = "packages.bib")
```

Random sampling from a fixed set is used in many areas of statistical computing.
The performance of this operation can be critical, especially when the sampled set is large.
The fast RNGs provided in this package make very fast sampling possible when combined with suitably fast algorithms.

## Benchmarks

By combining fast RNGs with a fast methods for creating [integers in a range](https://www.pcg-random.org/posts/bounded-rands.html) one gets good performance for sampling with replacement:

```{r replacement, eval=evaluate}
library(dqrng)
n <- 1e6
size <- 1e4
bm <- bench::mark(sample.int(n, size, replace = TRUE),
                  sample.int(1e4*n, size, replace = TRUE),
                  dqsample.int(n, size, replace = TRUE),
                  dqsample.int(1e4*n, size, replace = TRUE),
                  check = FALSE)
```
```{r, echo=FALSE}
if (evaluate) {
  saveRDS(bm, "data/replacement.RDS")
} else {
  bm <- readRDS("data/replacement.RDS")
}
knitr::kable(bm[, 1:5])
```

Note that sampling from `10^10` integers triggers "[long-vector support](https://stat.ethz.ch/R-manual/R-devel/library/base/html/LongVectors.html)" in R.

When sampling _without_ replacement one has to consider an appropriate algorithm for making sure that no entry is repeated. When more than 50% of the population are sampled, dqrng shuffles an appropriate part of the full list and returns that. The algorithm used in R is similar but dqrng has the edge with respect to performance:

```{r no-replacement-high, eval=evaluate}
library(dqrng)
n <- 1e6
size <- 6e5
bm <- bench::mark(sample.int(n, size),
                  dqsample.int(n, size),
                  check = FALSE, min_iterations = 50)
```
```{r, echo=FALSE}
if (evaluate) {
  saveRDS(bm, "data/no-replacement-high.RDS")
} else {
  bm <- readRDS("data/no-replacement-high.RDS")
}
knitr::kable(bm[, 1:5])
```

For lower sampling ratios a set based rejection sampling algorithm is used by dqrng. In principle, R can make use of a similar algorithm based on a hashset. However, it is only used for larger input vectors even though it is faster than the default method. The algorithm in dqrng, which is based on a [bitset](https://lemire.me/blog/2012/11/13/fast-sets-of-integers/), is even faster, though:

```{r no-replacement-medium, eval=evaluate}
library(dqrng)
n <- 1e6
size <- 1e4
bm <- bench::mark(sample.int(n, size),
                  sample.int(n, size, useHash = TRUE),
                  dqsample.int(n, size),
                  check = FALSE)
```
```{r, echo=FALSE}
if (evaluate) {
  saveRDS(bm, "data/no-replacement-medium.RDS")
} else {
  bm <- readRDS("data/no-replacement-medium.RDS")
}
knitr::kable(bm[, 1:5])
```

As one decreases the sampling rate even more, dqrng switches to a hashset based rejection sampling. Both hashset based methods have similar performance and are much faster than R's default method.

```{r no-replacement-low, eval=evaluate}
library(dqrng)
n <- 1e6
size <- 1e2
bm <- bench::mark(sample.int(n, size),
                  sample.int(n, size, useHash = TRUE),
                  dqsample.int(n, size),
                  check = FALSE)
```
```{r, echo=FALSE}
if (evaluate) {
  saveRDS(bm, "data/no-replacement-low.RDS")
} else {
  bm <- readRDS("data/no-replacement-low.RDS")
}
knitr::kable(bm[, 1:5])
```

For larger sampling ranges R uses the hashset by default, though `dqsample.int` is still faster:

```{r no-replacement-long, eval=evaluate}
library(dqrng)
n <- 1e10
size <- 1e5
bm <- bench::mark(sample.int(n, size),
                  dqsample.int(n, size),
                  check = FALSE)
```
```{r, echo=FALSE}
if (evaluate) {
  saveRDS(bm, "data/no-replacement-long.RDS")
} else {
  bm <- readRDS("data/no-replacement-long.RDS")
}
knitr::kable(bm[, 1:5])
```

The case of of weighted sampling is more complicated.
In addition to with or without replacement, one also has to consider the distribution of weights:
Is it fairly even as produced by `dqrunif()`?
Or is the weight concentrated in one or only a few possibilities?
Here only a few examples for weighted sampling both with replacement:

```{r weighted-replacement, eval=evaluate}
n <- 1e6
size <- 1e4
prob <- dqrunif(n)
bm <- bench::mark(sample.int(n, size, replace = TRUE, prob = prob),
                  dqsample.int(n, size, replace = TRUE, prob = prob),
                  check = FALSE)
```
```{r, echo=FALSE}
if (evaluate) {
  saveRDS(bm, "data/weighted-replacement.RDS")
} else {
  bm <- readRDS("data/weighted-replacement.RDS")
}
knitr::kable(bm[, 1:5])
```

And without replacement:

```{r weighted-no-replacement, eval=evaluate}
bm <- bench::mark(sample.int(n, size, prob = prob),
                  dqsample.int(n, size, prob = prob),
                  check = FALSE)
```
```{r, echo=FALSE}
if (evaluate) {
  saveRDS(bm, "data/weighted-no-replacement.RDS")
} else {
  bm <- readRDS("data/weighted-no-replacement.RDS")
}
knitr::kable(bm[, 1:5])
```

Especially for weighted sampling without replacement the performance advantage compared with R's default methods can be particularly large.

## Technicalities

The following methods are used for sampling without replacement. The algorithms are presented in R-like pseudo code, even though the real implementation is in C++. For sampling rates above 50%, a partial [Fisher-Yates shuffle](https://en.wikipedia.org/wiki/Fisher%E2%80%93Yates_shuffle) is used:

```{r, eval=FALSE}
no_replace_shuffle <- function(m, size) {
  tmp <- seq_len(m)
  for (i in seq_len(size))
    swap(tmp[i], tmp[i + random_int(m-i)])
  tmp[1:size]
}
```

where `random_int(m-i)` returns a random integer in `[0, m-i]`. Since the full population is kept in memory, this method is only suitable for high selection rates. One could expect that [reservoir sampling](https://en.wikipedia.org/wiki/Reservoir_sampling) should work well for lower selection rates. However, in my tests set based algorithms were faster:

```{r, eval=FALSE}
no_replace_set <- function(m, size) {
  result <- vector(mode = "...", length = size) # integer or numeric
  elems <- new(set, m, size) # set object for storing n objects out of m possible values
  for (i in seq_len(size))
    while (TRUE) {
      v = random_int(m)
      if (elems.insert(v)) {
        result[i] = v
        break
      }
    }
  result
}
```

Here `elems.insert(v)` returns `TRUE` if the insert was successful, i.e. `v` was not in `elems` before, and `FALSE` otherwise. There are different strategies for implementing such a set. For intermediate sampling rates (currently between 0.1% and 50%) dqrng uses a bitset, i.e. a vector of `n` bits each representing one of the possible values. For lower sampling rates the memory usage of this algorithm is to expensive, which is why a hashset^[For the specialists: Open addressing with a power-of-two size between 1.5 and 3 times `size`, identity hash function for the stored integers and quadratic probing.] is used, since there the used memory scales with `size` and not with `n`. One could expect that [Robert Floyd's sampling algorithm](https://stackoverflow.com/a/2394292/8416610) [@bentley1987] would be superior, but this was not the case in my tests, probably because it requires a final shuffling of the result to get a random _permutation_ instead of a random _combination_.

So far only un-weighted sampling was considered.
However, the some of the algorithms can be used for weighted sampling, when `random_int(n)`, which uniformly selects one of `n` elements, is replaced with a function that selects one of `n` elements based on their weights.
One such algorithm is stochastic acceptance [@lipowski2012], which is particularly well suited in combination with fast RNGs:

> 1. Select randomly one of the individuals (say, $i$). The selection is done with uniform probability $1/N$, which does not depend on the individual’s fitness $w_i$.
> 2. With probability $w_i/w_\max$, where $w_\max = \max\{w_i\}_{i=1}^N$ is the maximal fitness in the population, the selection is accepted. Otherwise, the procedure is repeated from step 1 (i.e., in the case of rejection, another selection attempt is made).

For sampling with replacement the above algorithm can be directly applied.
For sampling without replacement together with the set based rejection sampling, one might consider updating $w_\max$ whenever the element with the maximum weight has been selected.
However, this is not necessary since one can stick to the original $w_\max$ at the cost of a slightly reduced acceptance rate.

The stochastic acceptance method works well when the weights are more or less uniform.
However, the performance degrades quickly when the weight is concentrated on one or only a few items.
In this case, the alias method is used, which has been originally suggested by Walker [-@walker1974; -@walker1977] in the efficient formulation of @vose1991, which is also used by R in certain cases.
@keithschwarz2011 gives a detailed introduction into this method.

The final case would be high sampling rates without replacement.
For this Algorithm A from @efraimidis2006 in the exponential formulation of Müller [-@R-wrswoR; -@muller2016] has been implemented.

## Refernces
